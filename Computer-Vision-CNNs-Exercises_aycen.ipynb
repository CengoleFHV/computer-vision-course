{"cells":[{"cell_type":"markdown","metadata":{"id":"09A9OrUFAxPv"},"source":["# Computer Vision CNNs Notebook\n","\n","This notebook contains exercises for the computer vision CNNs material."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import pickle, bz2\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Saved under kth_tips (1).pbz2\n"]}],"source":["!python -m wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/kth_tips.pbz2 -o kth_tips.pbz2"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"w-95T8psAxPw","outputId":"a25064d9-b9e8-4b9b-e1e1-d2c64174d6df"},"outputs":[],"source":["with bz2.BZ2File('./kth_tips.pbz2', 'rb') as fd:\n","    kth_dataset = pickle.load(fd)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(kth_dataset['X'], kth_dataset['Y'], random_state=12345)\n","Y_train = tf.keras.utils.to_categorical(Y_train)\n","Y_test = tf.keras.utils.to_categorical(Y_test)\n","\n","X_train = X_train / 255\n","X_test = X_test / 255"]},{"cell_type":"markdown","metadata":{"id":"MSyxa1lrAxPw"},"source":["### Exercise 1 - Classify Image Dataset with Conventional Neural Network\n","\n","I have provided you with an image dataset (the same we used in the Local Binary Patterns exercise). Your images are in\n","X_train, X_test while youre labels are in Y_train, Y_test.\n","\n","Your job is to train a neural network on the pixel values to classify the images. To do so:\n","* Reshape the images from $(n,200,200)$ to $(n,40000)$\n","* Create a neural network model using keras\n","* Train your neural network on the (X_train, Y_train) and validate on the (X_test, Y_test) data.\n","* Compare your results with the LBP-results (if you have them)\n","* Explain what happened\n","\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(607, 200, 200) (607, 10) (203, 200, 200) (203, 10)\n","(607, 40000) (607, 10) (203, 40000) (203, 10)\n"]}],"source":["print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n","\n","# 1) Reshape the images from (n,200,200) to (n,40000)\n","X_train = X_train.reshape(X_train.shape[0], 40000)\n","X_test = X_test.reshape(X_test.shape[0], 40000)\n","\n","print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# 2) Create a neural network model with keras\n","nn = tf.keras.models.Sequential()\n","nn.add(Dense(512, input_dim=40000, activation='relu'))\n","nn.add(Dense(256, activation='relu'))\n","nn.add(Dense(10, activation='relu'))"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.1361 - loss: 7.6294 - val_accuracy: 0.1281 - val_loss: 7.0174\n","Epoch 2/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.1091 - loss: 7.4997 - val_accuracy: 0.0985 - val_loss: 7.0139\n","Epoch 3/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0902 - loss: 7.6250 - val_accuracy: 0.1084 - val_loss: 7.0131\n","Epoch 4/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1270 - loss: 7.8557 - val_accuracy: 0.0985 - val_loss: 7.0159\n","Epoch 5/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1061 - loss: 7.5463 - val_accuracy: 0.0936 - val_loss: 7.0149\n","Epoch 6/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1120 - loss: 7.8742 - val_accuracy: 0.1182 - val_loss: 7.0108\n","Epoch 7/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1086 - loss: 7.8715 - val_accuracy: 0.1034 - val_loss: 7.0117\n","Epoch 8/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1184 - loss: 7.6587 - val_accuracy: 0.1133 - val_loss: 7.0048\n","Epoch 9/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1099 - loss: 7.2678 - val_accuracy: 0.0936 - val_loss: 7.0087\n","Epoch 10/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1149 - loss: 7.4048 - val_accuracy: 0.0985 - val_loss: 6.9944\n","Epoch 11/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.1812 - loss: 7.5909 - val_accuracy: 0.1823 - val_loss: 6.9573\n","Epoch 12/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1674 - loss: 7.4235 - val_accuracy: 0.1034 - val_loss: 7.0006\n","Epoch 13/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1674 - loss: 7.6802 - val_accuracy: 0.2020 - val_loss: 6.9674\n","Epoch 14/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.2081 - loss: 7.7302 - val_accuracy: 0.2020 - val_loss: 6.9087\n","Epoch 15/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.2360 - loss: 7.7273 - val_accuracy: 0.1724 - val_loss: 7.0894\n","Epoch 16/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1916 - loss: 7.2991 - val_accuracy: 0.2118 - val_loss: 6.9134\n","Epoch 17/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.2297 - loss: 7.6464 - val_accuracy: 0.2167 - val_loss: 6.9057\n","Epoch 18/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.2133 - loss: 7.4374 - val_accuracy: 0.2266 - val_loss: 6.9078\n","Epoch 19/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.3102 - loss: 7.2242 - val_accuracy: 0.0985 - val_loss: 8.4382\n","Epoch 20/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1167 - loss: 8.6499 - val_accuracy: 0.1084 - val_loss: 8.3251\n","Epoch 21/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.1017 - loss: 9.1143 - val_accuracy: 0.1232 - val_loss: 8.3188\n","Epoch 22/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.1470 - loss: 9.4442 - val_accuracy: 0.1133 - val_loss: 8.3162\n","Epoch 23/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1641 - loss: 8.5204 - val_accuracy: 0.1133 - val_loss: 8.3131\n","Epoch 24/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1321 - loss: 9.3426 - val_accuracy: 0.1872 - val_loss: 8.3099\n","Epoch 25/25\n","\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.1600 - loss: 9.1277 - val_accuracy: 0.1626 - val_loss: 8.3037\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x25df7bffcd0>"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["# 3) train the neural network on the (X_train, Y_train) data and validate on (X_test, Y_test)\n","nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","nn.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=25, batch_size=32)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1944 - loss: 7.7934 \n","Accuracy: 16.26\n"]}],"source":["_, accuracy = nn.evaluate(X_test, Y_test)\n","print('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-yBWntoeAxPx"},"source":["### Exercise 2: Convolutional Layer\n","\n","In the cell below, I have provided you with a very simple CNN consisting only of a single convolutional layer.\n","You can access the weights (kernels) of a keras model by accessing the layer with:\n","\n","```python\n","kernel_list = cnn.layers[0].get_weights() # This gives us a list of kernels in the layer\n","```\n","\n","\n","Experiment with different parameters for ```filters, kernel_size``` and ```input_shape``` and have\n","a look at the kernels to get a solid understanding of how these kernels a CNN uses look like.\n","\n","Answer the following questions:\n","* How are the kernel dimensions influenced by the input dimensions?\n","* How is the number of parameters in the network influenced by the input dimension, the kernel size and the number of filters?\n","* Explain and summarize your findings."]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv2D, InputLayer"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"Tr2L43GiAxPx","outputId":"1c1f4b0d-7744-4ab6-b4d4-7d78f5a89512"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_20\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │            \u001b[38;5;34m76\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │            \u001b[38;5;34m20\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["cnn = Sequential()\n","cnn.add(InputLayer(input_shape=(128,128,3)))\n","cnn.add(Conv2D(filters=1, kernel_size=(5,5)))\n","cnn.add(Conv2D(filters=2, kernel_size=(3,3)))\n","\n","cnn.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WlKkNEvtAxPy"},"source":["### Exercise 3 - Pooling Layer\n","\n","Create a convolutional neural network with two convolution layers followed by pooling layers (use either max pooling or average pooling).\n","\n","Experiment with different values for ```pool_size, strides``` and study how the output shape is affected.\n","Answer the following questions:\n","* What is the difference in output size between max pooling and average pooling? Why?\n","* What is the impact on the receptive field of a convolutional layer following a pooling layer?\n","* How is the amount of parameters affected by a pooling layer?\n"]},{"cell_type":"markdown","metadata":{"id":"Aq8s24x9AxPy"},"source":["### Exercise 4: Flatten and GlobalAveragePooling\n","\n","In this exercise we will study two methods we can use to transfrom multi-dimensional feature maps to one-dimensional feature maps as used as input for Dense layers.\n","\n","1. Create a convolution neural network using two convolutional layers.\n","    * Use a Flatten layer to transform your multi-dimensional feature map to a one-dimensional feature map.\n","    * Add a Dense output layer for a classification problem with 10 classes.\n","    * Where is the majority of the weights in your network architecture?\n","    * How could this influence the training of your network?\n","\n","2. Create a convolution neural network using two convolutional layers.\n","    * Use a GlobalAveragePooling2D layer to transform your multi-dimensional feature map to a one-dimensional feature map.\n","    * Create a network architecture for a classification problem with 10 classes without using a Dense layer.\n","    * Where is the majority of weights in your network architecture?\n","    * How could this influence the training of your network?\n","\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"D-t0HEuJAxPy"},"source":["### Exercise 5: Classification using CNNS\n","\n","\n","In the first exercise in this notebook we already downloaded and prepared the KTH-TIPS dataset we used previously for classification. We saw that using a conventional NN\n","did not work very well in this scenario.\n","\n","1. Create a CNN of your choice to train a classifier for the provided dataset.\n","    * Train on (X_train, Y_train), Validate on (X_test, Y_test) (you can specify validation_data in the ```model.fit()``` function of keras)\n","2. Compare using Flatten() and GlobalAveragePooling() for feeding your features into Dense layers\n","    * How does it effect the number of parameters, the accuracy of your results and the time used to train the models?\n","3. What is the highest accuracy you can reach with the smallest amount of parameters?\n","\n","\n"]}],"metadata":{"celltoolbar":"Edit Metadata","colab":{"provenance":[{"file_id":"https://github.com/shegenbart/Jupyter-Exercises/blob/main/Computer-Vision-CNNs/Computer-Vision-CNNs-Exercises.ipynb","timestamp":1713708935270}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
